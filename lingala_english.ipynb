{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import pathlib\n",
    "\n",
    "path_to_file = pathlib.Path(\"C:/Users/Langat Kevin/Documents/New Bible Pair/lingala_english.txt\")\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lingala english'\n",
      " 'Kasi mabele ezalaki kaka bongobongo mpe ezalaki mpamba, mpe molili ezalaki likoló ya mai mozindo; nguya ya Nzambe ezalaki kotambola epai na epai likoló ya mai. Now the earth was formless and desolate, and there was darkness upon the surface of the watery deep, and Godâ€™s active force was moving about over the surface of the waters.'\n",
      " 'Nsima na yango, Nzambe amonaki ete pole ezalaki malamu, mpe Nzambe abandaki kokabola pole na molili. After that God saw that the light was good, and God began to divide the light from the darkness.'\n",
      " 'Na nsima, Nzambe alobaki ete: “Etando ezala kati na mai, mpe mai ekabwana na mibale.” Then God said: “Let there be an expanse between the waters, and let there be a division between the waters and the waters.”'\n",
      " 'Nzambe abengaki etando Likoló. Mpe mpokwa ekómaki mpe ntɔngɔ etanaki, wana mokolo ya mibale. God called the expanse Heaven. And there was evening and there was morning, a second day.'\n",
      " 'Nzambe abengaki mokili oyo ekauki Mabele, kasi mai oyo esanganaki abengaki yango Mbu. Mpe Nzambe amonaki ete ezali malamu. God called the dry land Earth, but the collecting of the waters, he called Seas. And God saw that it was good.'\n",
      " 'Mpe mabele ebandaki kobimisa matiti, milona oyo ebimisaka mboto na motindo na yango mpe banzete oyo ebotaka mbuma ná mboto na kati na yango. Bongo Nzambe amonaki ete ezali malamu. And the earth began to produce grass, seed-bearing plants and trees yielding fruit along with seed, according to their kinds. Then God saw that it was good.'\n",
      " 'Na nsima, Nzambe alobaki ete: “Bingɛngiseli ezala na etando ya likoló mpo na kosala ete moi ná butu ekabwana, mpe ekozala bilembo mpo na bileko, mpo na mikolo mpe mpo na bambula. Then God said: â€œLet there be luminaries in the expanse of the heavens to make a division between the day and the night, and they will serve as signs for seasons and for days and years.'\n",
      " 'Mpe Nzambe abandaki kosala bingɛngiseli mibale ya minene: oyo eleki monene mpo engɛnga na moi mpe oyo eleki moke mpo engɛnga na butu; asalaki mpe minzoto. And God went on to make the two great luminaries, the greater luminary for dominating the day and the lesser luminary for dominating the night, and also the stars.'\n",
      " 'mpe mpo engɛnga na moi mpe na butu mpe mpo na kosala ete pole ná molili ekabwana. Bongo Nzambe amonaki ete ezali malamu. and to dominate by day and by night and to make a division between the light and the darkness. Then God saw that it was good.'\n",
      " 'Na nsima, Nzambe alobaki ete: “Mai etonda mpenza kotonda na banyama, mpe bikelamu oyo epumbwaka epumbwa likoló ya mabele na etando ya likoló.” Then God said: “Let the waters swarm with living creatures, and let flying creatures fly above the earth across the expanse of the heavens.”'\n",
      " 'Bongo Nzambe apambolaki yango, alobaki ete: “Bóbota mpe bókóma mingi mpe bótondisa mai ya mbu, mpe tiká bikelamu oyo epumbwaka ekóma mingi na mabele.” With that God blessed them, saying: “Be fruitful and become many and fill the waters of the sea, and let the flying creatures become many in the earth.”'\n",
      " 'Na nsima, Nzambe alobaki ete: “Mabele ebimisa banyama na mitindo na yango: nyama ya mboka mpe nyama oyo ebendanaka na libumu mpe nyama ya zamba oyo ezali na mabele, na motindo na yango.” Esalemaki bongo. Then God said: “Let the earth bring forth living creatures according to their kinds, domestic animals and creeping animals and wild animals of the earth according to their kinds.” And it was so.'\n",
      " 'Na nsima, Nzambe alobaki ete: “Tósala moto na elilingi na biso, azala lokola biso, mpe bango bázala na bokonzi likoló ya mbisi ya mbu, bikelamu oyo epumbwaka na likoló, nyama ya mboka, mabele mobimba mpe likoló ya nyama mosusu nyonso oyo ebendanaka na libumu na mabele.” Then God said: “Let us make man in our image, according to our likeness, and let them have in subjection the fish of the sea and the flying creatures of the heavens and the domestic animals and all the earth and every creeping animal that is moving on the earth.”'\n",
      " 'Lisusu, Nzambe apambolaki bango, mpe Nzambe alobaki na bango ete: “Bóbota mpe bókóma mingi, bótondisa mabele mpe bótya yango na nse na bino, mpe bózala na bokonzi likoló ya mbisi ya mbu, bikelamu oyo epumbwaka na likoló, mpe ekelamu nyonso oyo ezali na bomoi, oyo ezali kotambola na mabele.” Further, God blessed them, and God said to them: “Be fruitful and become many, fill the earth and subdue it, and have in subjection the fish of the sea and the flying creatures of the heavens and every living creature that is moving on the earth.”'\n",
      " 'Mpe epai ya nyama nyonso ya zamba oyo ezali na mabele mpe epai ya ekelamu nyonso oyo epumbwaka na likoló mpe epai ya eloko nyonso oyo etambolaka na mabele oyo ezali na bomoi, napesi milona nyonso ya mobesu ete ezala bilei.” Mpe esalemaki bongo. And to every wild animal of the earth and to every flying creature of the heavens and to everything moving on the earth in which there is life, I have given all green vegetation for food.” And it was so.'\n",
      " 'Ndenge wana nde mosala ya kozalisa likoló mpe mabele ná biloko na yango nyonso esilaki. Thus the heavens and the earth and everything in them were completed.'\n",
      " 'Mpe na nsima Nzambe abandaki kopambola mokolo ya nsambo mpe kosantisa yango, mpo na mokolo yango Nzambe azali kopema na misala na ye nyonso ya kozalisa, na misala nyonso oyo akanaki kosala. And God went on to bless the seventh day and to declare it sacred, for on it God has been resting from all the work that he has created, all that he purposed to make.'\n",
      " 'Nzete ya moke ata moko te ezalaki naino na mabele mpe molona ata moko te ebimaki naino na esobe, mpo Yehova Nzambe anɔkisaki naino mbula na mabele te mpe moto azalaki naino te mpo na kosala bilanga na mabele. No bush of the field was yet on the earth and no vegetation of the field had begun sprouting, because Jehovah God had not made it rain on the earth and there was no man to cultivate the ground.'\n",
      " 'Na nsima, Yehova Nzambe asalaki moto na putulu ya mabele mpe afulaki na kati ya zolo na ye mpema ya bomoi, mpe moto akómaki ekelamu ya bomoi. And Jehovah God went on to form the man out of dust from the ground and to blow into his nostrils the breath of life, and the man became a living person.'\n",
      " 'Bongo Yehova Nzambe akolisaki na mabele nzete nyonso ya kitoko na kotala mpe malamu na kolya mpe lisusu akolisaki nzete ya bomoi na katikati ya elanga mpe nzete ya koyeba malamu ná mabe. Thus Jehovah God made to grow out of the ground every tree that was pleasing to look at and good for food and also the tree of life in the middle of the garden and the tree of the knowledge of good and bad.'\n",
      " 'Oyo ya liboso nkombo na yango Pishone; yango nde ezingi mokili mobimba ya Havila, esika wolo ezali. The name of the first is Piʹshon; it is the one encircling the entire land of Havʹi·lah, where there is gold.'\n",
      " 'Ebale ya mibale nkombo na yango Gihone; yango nde ezingi mokili mobimba ya Kushi. The name of the second river is Gi\\xa0πhon; it is the one encircling the entire land of Cush.'\n",
      " 'Yehova Nzambe azwaki moto mpe atyaki ye na elanga ya Edene mpo na kobongisa yango mpe kobatela yango. Jehovah God took the man and settled him in the garden of EÊ¹den to cultivate it and to take care of it.'\n",
      " 'Kasi mbuma ya nzete ya koyeba malamu ná mabe, komeka kolya yango te, mpo mokolo okolya yango okokufa mpenza.” But as for the tree of the knowledge of good and bad, you must not eat from it, for in the day you eat from it you will certainly die.”'\n",
      " 'Nzokande, na putulu ya mabele, Yehova Nzambe azalaki kosala nyama nyonso ya zamba oyo ezali na mokili mpe ekelamu nyonso oyo epumbwaka na likoló. Mpe abandaki koya na yango epai ya moto mpo na komona ndenge akopesa mokomoko na yango nkombo; mpe ekelamu nyonso ya bomoi ezalaki kozwa nkombo oyo moto azalaki kopesa yango. Now Jehovah God had been forming from the ground every wild animal of the field and every flying creature of the heavens, and he began bringing them to the man to see what he would call each one; and whatever the man would call each living creature, that became its name.'\n",
      " 'Na yango Yehova Nzambe asalaki ete mobali alala mpɔngi makasi, mpe ntango azalaki kolala, Nzambe alongolaki mokuwa moko ya mopanzi na ye mpe azipaki mosuni na esika yango. So Jehovah God caused the man to fall into a deep sleep, and while he was sleeping, he took one of his ribs and then closed up the flesh over its place.'\n",
      " 'Bongo mobali alobaki ete: “Oyo nde mpenza mokuwa ya mikuwa na ngai Mpe mosuni ya mosuni na ngai. Akobengama Mwasi, Mpo auti na mobali.” Then the man said: “This is at last bone of my bones And flesh of my flesh. This one will be called Woman, Because from man she was taken.”'\n",
      " 'Bango mibale bazalaki bolumbu, mobali ná mwasi na ye; kasi bazalaki koyoka nsɔni te. And both of them continued to be naked, the man and his wife; yet they were not ashamed.'\n",
      " 'Bongo mwasi alobaki na nyoka ete: “Tokoki kolya mbuma ya banzete ya elanga. At this the woman said to the serpent: â€œWe may eat of the fruit of the trees of the garden.'\n",
      " 'Bongo nyoka alobaki na mwasi ete: “Bokokufa soki moke te. At this the serpent said to the woman: â€œYou certainly will not die.'\n",
      " 'Na yango, mwasi amonaki ete mbuma ya nzete yango ezalaki malamu mpo na kolya mpe ezalaki kosepelisa miso, ɛɛ, nzete yango ezalaki kitoko na kotala. Bongo abukaki mbuma na yango mpe alyaki. Na nsima apesaki mpe mobali na ye ntango azalaki elongo na ye mpe ye alyaki. Consequently, the woman saw that the tree was good for food and that it was something desirable to the eyes, yes, the tree was pleasing to look at. So she began taking of its fruit and eating it. Afterward, she also gave some to her husband when he was with her, and he began eating it.'\n",
      " 'Na nsima bayokaki mongongo ya Yehova Nzambe wana azalaki kotambola na elanga na ntango oyo mwa mopɛpɛ kitoko epɛpaka na mokolo, mpe mobali ná mwasi na ye bakendaki komibomba na katikati ya banzete ya elanga mpo Yehova Nzambe amona bango te. Later they heard the voice of Jehovah God as he was walking in the garden about the breezy part of the day, and the man and his wife hid from the face of Jehovah God among the trees of the garden.'\n",
      " 'Nsukansuka alobaki ete: “Nayokaki mongongo na yo na elanga, kasi nabangaki mpo nazalaki bolumbu, yango wana namibombaki.” Finally he said: “I heard your voice in the garden, but I was afraid because I was naked, so I hid myself.”'\n",
      " 'Mobali alobaki ete: “Mwasi oyo opesaki ngai, apesaki ngai mbuma ya nzete yango, mpe nalyaki.” The man said: “The woman whom you gave to be with me, she gave me fruit from the tree, so I ate.”'\n",
      " 'Na nsima, Yehova Nzambe alobaki na nyoka ete: “Lokola osali bongo, olakelami mabe na kati ya banyama nyonso ya mboka mpe na kati ya banyama nyonso ya zamba. Okotambola na libumu na yo, mpe okolya putulu mikolo nyonso ya bomoi na yo. Then Jehovah God said to the serpent: â€œBecause you have done this, you are the cursed one out of all the domestic animals and out of all the wild animals of the field. On your belly you will go, and you will eat dust all the days of your life.'\n",
      " 'Alobaki na mwasi ete: “Nakobakisa mpenza mpasi ya zemi na yo; okobota bana na mpasi, mpe okozala na mposa makasi ya mobali na yo, mpe ye akokonza yo.” To the woman he said: “I will greatly increase the pain of your pregnancy; in pain you will give birth to children, and your longing will be for your husband, and he will dominate you.”'\n",
      " 'Ekobimisela yo banzubɛ mpe matiti ya nsendensende, mpe okolya milona ya mokili. It will grow thorns and thistles for you, and you must eat the vegetation of the field.'\n",
      " 'Nsima na yango, Adama apesaki mwasi na ye nkombo Eva, mpo asengelaki kokóma mama ya moto nyonso oyo azali na bomoi. After this Adam named his wife Eve, because she was to become the mother of everyone living.'\n",
      " 'Na nsima, Yehova Nzambe alobaki ete: “Awa moto akómi lokola moko na biso na ndenge ayebi malamu ná mabe. Sikoyo, mpo asembola lobɔkɔ na ye te mpe azwa lisusu mbuma ya nzete ya bomoi mpe alya yango mpe azala na bomoi libela na libela,—” Jehovah God then said: “Here the man has become like one of us in knowing good and bad. Now in order that he may not put his hand out and take fruit also from the tree of life and eat and live forever,—”'\n",
      " 'Na yango, Nzambe abenganaki moto mpe atyaki bakeruba na ɛsti ya elanga ya Edene mpe mopanga, oyo mɔtɔ ezalaki kopela na ebende na yango, ezalaki kaka kobalukabaluka mpo na kokɛngɛla nzela ya nzete ya bomoi. So he drove the man out, and he posted at the east of the garden of EÊ¹den the cherubs and the flaming blade of a sword that was turning continuously to guard the way to the tree of life.'\n",
      " 'Na nsima, abotaki mpe ndeko na ye Abele. Abele akómaki mobateli ya bampate, kasi Kaina akómaki mosali-bilanga. Later she again gave birth, to his brother Abel. Abel became a shepherd of the flock, but Cain became a cultivator of the ground.'\n",
      " 'Kasi Abele amemaki bana ya liboso ya banyama ya etonga na ye, ná mafuta na yango. Yehova andimaki Abele ná likabo na ye, But Abel brought some firstlings of his flock, including their fat. While Jehovah looked with favor on Abel and on his offering,'\n",
      " 'Na nsima, Yehova alobaki na Kaina ete: “Mpo na nini osiliki makasi boye, mpe oyoki mabe mpenza? Then Jehovah said to Cain: â€œWhy are you so angry and dejected?'\n",
      " 'Nsima na yango, Kaina alobaki na ndeko na ye Abele ete: “Yaká tókende na bilanga.” Mpe ntango bazalaki na bilanga, Kaina akwelaki ndeko na ye Abele mpe abomaki ye. After that Cain said to his brother Abel: “Let us go over into the field.” So while they were in the field, Cain assaulted his brother Abel and killed him.'\n",
      " 'Mpe Nzambe alobaki ete: “Likambo nini osali? Yoká! Makila ya ndeko na yo ezali kobelela ngai uta na mabele. At this He said: “What have you done? Listen! Your brother’s blood is crying out to me from the ground.'\n",
      " 'Ntango okolona biloko na mabele, ekobota te na ndenge esengeli. Okobanda koyengayenga mpe kokimakima na mabele.” When you cultivate the ground, it will not give you back its produce. You will become a wanderer and a fugitive in the earth.”'\n",
      " 'Lelo oyo ozali kobengana ngai na mokili oyo, nakobombana mpo namonana te liboso ya elongi na yo; nakokóma koyengayenga mpe kokimakima na mabele, mpe moto oyo akomona ngai akoboma ngai mpenza.” Today you are driving me from the land, and I will be hidden from your face; and I will become a wanderer and a fugitive on the earth, and anyone who finds me will certainly kill me.”'\n",
      " 'Na nsima, Kaina alongwaki liboso ya Yehova mpe akendaki kofanda na “mokili ya moto oyo akimá,” na ɛsti ya Edene. Then Cain went away from before Jehovah and took up residence in the land of Exile, to the east of EÊ¹den.'\n",
      " 'Na nsima, Enoka abotaki Irade. Irade abotaki Mehuyaele, Mehuyaele abotaki Metushaele, mpe Metushaele abotaki Lameke. Later Iʹrad was born to Eʹnoch. And Iʹrad became father to Me·huʹja·el, and Me·huʹja·el became father to Me·thuʹsha·el, and Me·thuʹsha·el became father to Laʹmech.'] \n",
      "\n",
      " ['Na ebandeli Nzambe azalisaki likoló mpe mabele. In the beginning God created the heavens and the earth.'\n",
      " 'Mpe Nzambe alobaki ete: “Pole ezala.” Bongo pole ezalaki. And God said: “Let there be light.” Then there was light.'\n",
      " 'Nzambe abengaki pole Moi, kasi abengaki molili Butu. Mpe mpokwa ekómaki mpe ntɔngɔ etanaki, wana mokolo ya liboso. God called the light Day, but the darkness he called Night. And there was evening and there was morning, a first day.'\n",
      " 'Bongo Nzambe abandaki kosala etando mpe asalaki ete mai oyo ezalaki na nse ya etando ekabwana na mai oyo ezalaki na likoló ya etando. Mpe esalemaki bongo. Then God went on to make the expanse and divided the waters beneath the expanse from the waters above the expanse. And it was so.'\n",
      " 'Na nsima, Nzambe alobaki ete: “Mai oyo ezali na nse ya likoló esangana esika moko, mpe mabele oyo ekauki emonana.” Mpe esalemaki bongo. Then God said: “Let the waters under the heavens be collected together into one place, and let the dry land appear.” And it was so.'\n",
      " 'Na nsima, Nzambe alobaki ete: “Mabele ebimisa matiti, milona oyo ebimisaka mboto. Ebimisa mpe banzete oyo ebotaka mbuma na motindo na yango, oyo ebotaka na mabele mbuma ná mboto na kati na yango.” Mpe esalemaki bongo. Then God said: “Let the earth cause grass to sprout, seed-bearing plants and fruit trees according to their kinds, yielding fruit along with seed on the earth.” And it was so.'\n",
      " 'Mpe mpokwa ekómaki mpe ntɔngɔ etanaki, wana mokolo ya misato. And there was evening and there was morning, a third day.'\n",
      " 'Ekozala bingɛngiseli na etando ya likoló mpo na kongɛngisa mabele.” Mpe esalemaki bongo. They will serve as luminaries in the expanse of the heavens to shine upon the earth.” And it was so.'\n",
      " 'Na bongo Nzambe atyaki yango na etando ya likoló mpo na kongɛngisa mabele, Thus God put them in the expanse of the heavens to shine upon the earth'\n",
      " 'Mpe mpokwa ekómaki mpe ntɔngɔ etanaki, wana mokolo ya minei. And there was evening and there was morning, a fourth day.'\n",
      " 'Mpe Nzambe azalisaki banyama ya mineneminene ya mbu mpe banyama nyonso oyo etambolaka mpe oyo etondanaka na mitindo na yango mpe motindo ya ekelamu nyonso oyo epumbwaka oyo ezali na mapapu. Mpe Nzambe amonaki ete ezali malamu. And God created the great sea creatures and all living creatures that move and swarm in the waters according to their kinds and every winged flying creature according to its kind. And God saw that it was good.'\n",
      " 'Mpe mpokwa ekómaki mpe ntɔngɔ etanaki, wana mokolo ya mitano. And there was evening and there was morning, a fifth day.'\n",
      " 'Mpe Nzambe abandaki kosala nyama ya zamba oyo ezali na mabele, na mitindo na yango mpe nyama ya mboka na mitindo na yango mpe mitindo ya nyama nyonso oyo ebendanaka na libumu na mabele. Nzambe amonaki ete ezali malamu. And God went on to make the wild animals of the earth according to their kinds and the domestic animals according to their kinds and all the creeping animals of the ground according to their kinds. And God saw that it was good.'\n",
      " 'Mpe Nzambe abandaki kozalisa moto na elilingi na ye; na elilingi ya Nzambe azalisaki ye. Azalisaki bango mobali ná mwasi. And God went on to create the man in his image, in Godâ€™s image he created him; male and female he created them.'\n",
      " 'Na nsima Nzambe alobaki ete: “Talá napesi bino milona nyonso oyo ebotaka mboto, oyo ezali na mabele mobimba mpe nzete nyonso oyo ezali na mbuma oyo ebotaka mboto. Tiká yango ezala bilei na bino. Then God said: â€œHere I have given to you every seed-bearing plant that is on the entire earth and every tree with seed-bearing fruit. Let them serve as food for you.'\n",
      " 'Nsima na yango Nzambe amonaki eloko nyonso oyo asalaki, mpe talá! ezalaki malamu mingi. Mpe mpokwa ekómaki mpe ntɔngɔ etanaki, wana mokolo ya motoba. After that God saw everything he had made, and look! it was very good. And there was evening and there was morning, a sixth day.'\n",
      " 'Mpe na mokolo ya nsambo, Nzambe asilisaki mosala na ye oyo asalaki mpe abandaki kopema na mokolo ya nsambo na misala na ye nyonso oyo asalaki. And by the seventh day, God had completed the work that he had been doing, and he began to rest on the seventh day from all his work that he had been doing.'\n",
      " 'Oyo ezali lisolo ya makambo etali likoló mpe mabele na ntango oyo yango ezalisamaki, na mokolo oyo Yehova Nzambe asalaki mabele mpe likoló. This is a history of the heavens and the earth in the time they were created, in the day that Jehovah God made earth and heaven.'\n",
      " 'Kasi londende ezalaki kouta na mabele mpe ezalaki kopɔlisa etando mobimba ya mokili. But a mist would go up from the earth, and it watered the entire surface of the ground.'\n",
      " 'Lisusu, Yehova Nzambe alonaki elanga na Edene, na ngámbo ya ɛsti, mpe kuna atyaki moto oyo ye asalaki. Further, Jehovah God planted a garden in EÊ¹den, toward the east; and there he put the man whom he had formed.'\n",
      " 'Sikoyo ebale moko ezalaki kobima na Edene mpo na kopɔlisa elanga. Na nsima ekabwanaki mpe ekómaki bibale minei. Now there was a river flowing out of EÊ¹den to water the garden, and from there it divided into four rivers.'\n",
      " 'Wolo ya mokili yango ezali malamu. Bedeliume mpe libanga ya oniksi ezali mpe kuna. The gold of that land is good. Bdellium gum and onyx stone are also there.'\n",
      " 'Ebale ya misato nkombo na yango Hidekele; yango nde ezali kotíyola na ɛsti ya Asiri. Mpe ebale ya minei ezali Efrate. The name of the third river is Hidʹde·kel; it is the one going to the east of As·syrʹi·a. And the fourth river is the Eu·phraʹtes.'\n",
      " 'Lisusu Yehova Nzambe apesaki moto mobeko oyo: “Mbuma ya nzete nyonso ya elanga okoki kolya yango ndenge olingi. Jehovah God also gave this command to the man: â€œFrom every tree of the garden you may eat to satisfaction.'\n",
      " 'Na nsima Yehova Nzambe alobaki ete: “Ezali malamu te moto azala kaka ye moko. Nakosalela ye mosungi, oyo akokani na ye.” Then Jehovah God said: “It is not good for the man to continue to be alone. I am going to make a helper for him, as a complement of him.”'\n",
      " 'Bongo moto apesaki nkombo na banyama nyonso ya mboka mpe bikelamu oyo epumbwaka na likoló mpe nyama nyonso ya zamba oyo ezali na mokili, kasi mosungi oyo akokani na ye azalaki te. So the man named all the domestic animals and the flying creatures of the heavens and every wild animal of the field, but for man there was no helper as a complement of him.'\n",
      " 'Mpe na mokuwa oyo alongolaki na mopanzi ya mobali, Yehova Nzambe asalaki mwasi mpe ayaki na ye epai ya mobali. And Jehovah God built the rib that he had taken from the man into a woman, and he brought her to the man.'\n",
      " 'Yango wana mobali akotika tata na ye ná mama na ye mpe akokangama na mwasi na ye mpe bakokóma mosuni moko. That is why a man will leave his father and his mother and he will stick to his wife, and they will become one flesh.'\n",
      " 'Sikoyo nyoka ezalaki ekɛngɛ koleka banyama nyonso ya zamba ya mokili oyo Yehova Nzambe asalaki. Na yango, alobaki na mwasi ete: “Nzambe alobaki na bino mpenza ete bosengeli te kolya mbuma ya nzete nyonso ya elanga?” Now the serpent was the most cautious of all the wild animals of the field that Jehovah God had made. So it said to the woman: “Did God really say that you must not eat from every tree of the garden?”'\n",
      " 'Kasi mpo na mbuma ya nzete oyo ezali na katikati ya elanga, Nzambe alobaki ete: ‘Bosengeli kolya yango te, bosengeli mpe kosimba yango te; soki bomeki bokokufa.’” But God has said about the fruit of the tree that is in the middle of the garden: ‘You must not eat from it, no, you must not touch it; otherwise you will die.’”'\n",
      " 'Mpo Nzambe ayebi ete mokolo kaka bokolya mbuma ya nzete yango, miso na bino ekofungwama mpe bokokóma lokola Nzambe: bokoyeba malamu ná mabe.” For God knows that in the very day you eat from it, your eyes will be opened and you will be like God, knowing good and bad.”'\n",
      " 'Bongo miso na bango mibale efungwamaki, mpe basosolaki ete bazalaki bolumbu. Na yango, batongaki nkasa ya figi mpe bamisalelaki biloko ya kozipa loketo. Then the eyes of both of them were opened, and they realized that they were naked. So they sewed fig leaves together and made loin coverings for themselves.'\n",
      " 'Mpe Yehova Nzambe azalaki kobenga mobali mpe koloba na ye ete: “Ozali wapi?” And Jehovah God kept calling to the man and saying to him: “Where are you?”'\n",
      " 'Na yango Nzambe alobaki ete: “Nani ayebisaki yo ete ozali bolumbu? Olei nde mbuma ya nzete oyo napesaki yo mobeko ete olya yango te?” At that he said: “Who told you that you were naked? Have you eaten from the tree from which I commanded you not to eat?”'\n",
      " 'Na nsima, Yehova Nzambe alobaki na mwasi ete: “Likambo nini osali?” Mwasi ayanolaki ete: “Nyoka akosaki ngai, yango wana nalyaki.” Jehovah God then said to the woman: “What is this you have done?” The woman replied: “The serpent deceived me, so I ate.”'\n",
      " 'Mpe nakotya bonguna kati na yo ná mwasi mpe kati na bana na yo ná mwana na ye. Ye akopanza motó na yo mpe yo okozokisa ye na litindi.” And I will put enmity between you and the woman and between your offspring and her offspring. He will crush your head, and you will strike him in the heel.”'\n",
      " 'Mpe alobaki na Adama ete: “Lokola oyoki mongongo ya mwasi na yo mpe olei mbuma ya nzete oyo napesaki yo mobeko oyo ete: ‘Osengeli kolya mbuma na yango te,’ mabele elakelami mabe mpo na yo. Mikolo nyonso ya bomoi na yo okolya na mpasi biloko oyo mabele ekobota. And to Adam he said: “Because you listened to your wife’s voice and ate from the tree concerning which I gave you this command, ‘You must not eat from it,’ cursed is the ground on your account. In pain you will eat its produce all the days of your life.'\n",
      " 'Na motoki ya elongi na yo okolya limpa tii okozonga na mabele, mpo outaki kuna. Mpo ozali putulu mpe okozonga na putulu.” In the sweat of your face you will eat bread until you return to the ground, for out of it you were taken. For dust you are and to dust you will return.”'\n",
      " 'Mpe Yehova Nzambe asalelaki Adama ná mwasi na ye bilamba milai ya mposo ya nyama mpo bálata. And Jehovah God made long garments from skins for Adam and for his wife, to clothe them.'\n",
      " 'Bongo Yehova Nzambe abenganaki ye na elanga ya Edene, mpo akende kosala bilanga na mabele esika azwamaki. With that Jehovah God expelled him from the garden of EÊ¹den to cultivate the ground from which he had been taken.'\n",
      " 'Adama asangisaki nzoto na mwasi na ye Eva, mpe Eva azwaki zemi. Ntango abotaki Kaina, alobaki ete: “Yehova asalisi ngai nabota mwana mobali.” Now Adam had sexual relations with his wife Eve, and she became pregnant. When she gave birth to Cain, she said: “I have produced a male child with the help of Jehovah.”'\n",
      " 'Nsima ya mwa ntango, Kaina amemelaki Yehova mwa likabo ya mbuma ya bilanga. After some time, Cain brought some fruits of the land as an offering to Jehovah.'\n",
      " 'kasi andimaki ata moke te Kaina ná likabo na ye. Na yango, Kaina asilikaki makasi, mpe ayokaki mabe mpenza. he did not look with any favor on Cain and on his offering. So Cain grew hot with anger and was dejected.'\n",
      " 'Soki obandi kosala makambo ya malamu, nakondima yo te? Kasi soki osali makambo ya malamu te, lisumu ebatami na porte, mpe ezali na mposa ya kokonza yo; kasi yo okokoka kolonga yango?” If you turn to doing good, will you not be restored to favor? But if you do not turn to doing good, sin is crouching at the door, and its craving is to dominate you; but will you get the mastery over it?‚Äù'\n",
      " 'Na nsima, Yehova alobaki na Kaina ete: “Wapi ndeko na yo Abele?” mpe ye alobaki ete: “Nayebi te. Ngai nde mobateli ya ndeko na ngai?” Later on, Jehovah said to Cain: “Where is your brother Abel?” and he said: “I do not know. Am I my brother’s guardian?”'\n",
      " 'Mpe sikoyo olakelami mabe, nabengani yo na mabele oyo efungoli monɔkɔ na yango mpo na komɛla makila ya ndeko na yo oyo lobɔkɔ na yo esopi. And now you are cursed in banishment from the ground that has opened its mouth to receive your brotherâ€™s blood from your hand.'\n",
      " 'Bongo Kaina alobaki na Yehova ete: “Etumbu mpo na libunga na ngai eleki monene mingi, nakokoka yango te. At this Cain said to Jehovah: â€œThe punishment for my error is too great to bear.'\n",
      " 'Na yango, Yehova alobaki na ye ete: “Yango wana moto oyo akoboma Kaina bakozongisela ye mabe mbala nsambo.” Bongo, Yehova atyaki elembo mpo na Kaina ete moto oyo akomona ye aboma ye te. So Jehovah said to him: “For that reason, anyone who kills Cain will suffer vengeance seven times.” So Jehovah set up a sign for Cain in order that no one finding him would strike him.'\n",
      " 'Na nsima, Kaina asangisaki nzoto na mwasi na ye mpe azwaki zemi mpe abotaki Enoka. Bongo abandaki kotonga engumba moko mpe apesaki yango nkombo ya mwana na ye Enoka. Afterward Cain had sexual relations with his wife, and she became pregnant and gave birth to Eʹnoch. Then he engaged in building a city and named the city after his son Eʹnoch.'\n",
      " 'Lameke azwaki basi mibale. Ya liboso nkombo na ye Ada mpe ya mibale Zila. Laʹmech took two wives for himself. The name of the first was Aʹdah, and the name of the second was Zilʹlah.']\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split(\"\\t\") for line in lines if len(line.split(\"\\t\")) == 2]\n",
    "    context = np.array([context for context, target in pairs])\n",
    "    target = np.array([target for context, target in pairs])\n",
    "    return context, target\n",
    "\n",
    "lingala_sentences, english_sentences = load_data(path_to_file)\n",
    "sentences = (lingala_sentences, english_sentences)\n",
    "print(lingala_sentences, \"\\n\\n\", english_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating Datasets for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True False False False  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      " False  True  True  True  True  True False  True False  True  True  True\n",
      "  True False  True False  True  True  True  True  True  True  True  True\n",
      "  True  True]\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(english_sentences)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(lingala_sentences),)) < 0.8\n",
    "print(is_train)\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (english_sentences[is_train], lingala_sentences[is_train])\n",
    "    )\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "val_raw = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (english_sentences[~is_train], lingala_sentences[~is_train])\n",
    "    )\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, r\"[^ a-z.?!,¿]\", \"\")\n",
    "    text = tf.strings.regex_replace(text, r\"[.?!,¿]\", r\" \\0 \")\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
    "    return text\n",
    "\n",
    "max_vocab_size = 12000\n",
    "\n",
    "english_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, output_mode='int', ragged=True\n",
    ")\n",
    "\n",
    "english_vectorizer.adapt(train_raw.map(lambda context, target: context))\n",
    "\n",
    "lingala_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, output_mode='int', ragged=True\n",
    ")\n",
    "\n",
    "lingala_vectorizer.adapt(train_raw.map(lambda context, target: target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Processing Text Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    context = english_vectorizer(context).to_tensor()\n",
    "    target = lingala_vectorizer(target)\n",
    "    targ_in = target[:, :-1].to_tensor()\n",
    "    targ_out = target[:, 1:].to_tensor()\n",
    "    return (context, targ_in), targ_out\n",
    "\n",
    "train_data = train_raw.map(lambda x, y: process_text(x, y), tf.data.AUTOTUNE).repeat()\n",
    "val_data = val_raw.map(lambda x, y: process_text(x, y), tf.data.AUTOTUNE).repeat()\n",
    "\n",
    "del train_raw\n",
    "del val_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Define Loss and Accuracy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "def masked_acc(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of sentences in english has shape: (41, 107)\n",
      "\n",
      "Encoder output has shape: (41, 107, 256)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 12000\n",
    "UNITS = 256\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=units, mask_zero=True)\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            merge_mode=\"sum\",\n",
    "            layer=tf.keras.layers.LSTM(units=units, return_sequences=True),\n",
    "        )\n",
    "\n",
    "    def call(self, context):\n",
    "        x = self.embedding(context)\n",
    "        x = self.rnn(x)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "for (to_translate, sr_translation), _ in train_data.take(1):\n",
    "    encoder_output = encoder(to_translate)\n",
    "    print(f'Tensor of sentences in english has shape: {to_translate.shape}\\n')\n",
    "    print(f'Encoder output has shape: {encoder_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Define Cross Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling EinsumDense.call().\n\n\u001b[1m{{function_node __wrapped____MklBatchMatMulV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [41,117,10496], In[1]: [256,256] 0 0 [Op:BatchMatMulV2] name: \u001b[0m\n\nArguments received by EinsumDense.call():\n  • inputs=tf.Tensor(shape=(41, 117, 41, 256), dtype=float32)\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m context_mask \u001b[38;5;241m=\u001b[39m context_mask[:, tf\u001b[38;5;241m.\u001b[39mnewaxis, tf\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[0;32m     22\u001b[0m target_mask \u001b[38;5;241m=\u001b[39m target_mask[:, tf\u001b[38;5;241m.\u001b[39mnewaxis, tf\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[1;32m---> 24\u001b[0m attention_result \u001b[38;5;241m=\u001b[39m \u001b[43mattention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr_translation_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor of contexts has shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor of translations has shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msr_translation_embed\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[51], line 9\u001b[0m, in \u001b[0;36mCrossAttention.call\u001b[1;34m(self, context, target, context_mask, target_mask)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, context, target, context_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 9\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd([target, attn_output])\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(x)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling EinsumDense.call().\n\n\u001b[1m{{function_node __wrapped____MklBatchMatMulV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [41,117,10496], In[1]: [256,256] 0 0 [Op:BatchMatMulV2] name: \u001b[0m\n\nArguments received by EinsumDense.call():\n  • inputs=tf.Tensor(shape=(41, 117, 41, 256), dtype=float32)\n  • training=None"
     ]
    }
   ],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, context, target, context_mask=None, target_mask=None):\n",
    "        attn_output, _ = self.mha(query=target, value=context, key=context, attention_mask=context_mask)\n",
    "        x = self.add([target, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "attention_layer = CrossAttention(UNITS)\n",
    "sr_translation_embed = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=UNITS, mask_zero=True)(sr_translation)\n",
    "\n",
    "context_mask = tf.cast(tf.sequence_mask(tf.reduce_sum(tf.cast(to_translate != 0, tf.int32), axis=1), maxlen=tf.shape(to_translate)[1]), dtype=tf.float32)\n",
    "target_mask = tf.cast(tf.sequence_mask(tf.reduce_sum(tf.cast(sr_translation != 0, tf.int32), axis=1), maxlen=tf.shape(sr_translation)[1]), dtype=tf.float32)\n",
    "\n",
    "# Ensure the mask is in the correct shape for MultiHeadAttention\n",
    "context_mask = context_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "target_mask = target_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "attention_result = attention_layer(encoder_output, sr_translation_embed, context_mask=context_mask, target_mask=target_mask)\n",
    "\n",
    "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
    "print(f'Tensor of translations has shape: {sr_translation_embed.shape}')\n",
    "print(f'Tensor of attention scores has shape: {attention_result.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:915: UserWarning: Layer 'cross_attention_5' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of contexts has shape: (41, 107, 256)\n",
      "Tensor of right-shifted translations has shape: (41, 117)\n",
      "Tensor of logits has shape: (41, 117, 12000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:915: UserWarning: Layer 'decoder_1' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=units, mask_zero=True)\n",
    "        self.pre_attention_rnn = tf.keras.layers.LSTM(units=units, return_sequences=True, return_state=True)\n",
    "        self.attention = CrossAttention(units)\n",
    "        self.post_attention_rnn = tf.keras.layers.LSTM(units=units, return_sequences=True)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=vocab_size, activation=tf.nn.log_softmax)\n",
    "\n",
    "    def call(self, context, target, state=None, return_state=False):\n",
    "        x = self.embedding(target)\n",
    "        x, hidden_state, cell_state = self.pre_attention_rnn(x, initial_state=state)\n",
    "        x = self.attention(context, x)\n",
    "        x = self.post_attention_rnn(x)\n",
    "        logits = self.output_layer(x)\n",
    "        if return_state:\n",
    "            return logits, [hidden_state, cell_state]\n",
    "        return logits\n",
    "\n",
    "decoder = Decoder(VOCAB_SIZE, UNITS)\n",
    "logits = decoder(encoder_output, sr_translation)\n",
    "\n",
    "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
    "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
    "print(f'Tensor of logits has shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Defining The Tranformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:915: UserWarning: Layer 'cross_attention_6' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:915: UserWarning: Layer 'decoder_2' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of sentences to translate has shape: (41, 107)\n",
      "Tensor of right-shifted translations has shape: (41, 117)\n",
      "Tensor of logits has shape: (41, 117, 12000)\n"
     ]
    }
   ],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, units)\n",
    "        self.decoder = Decoder(vocab_size, units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context, target = inputs\n",
    "        encoded_context = self.encoder(context)\n",
    "        logits = self.decoder(encoded_context, target)\n",
    "        return logits\n",
    "\n",
    "translator = Translator(VOCAB_SIZE, UNITS)\n",
    "logits = translator((to_translate, sr_translation))\n",
    "\n",
    "print(f'Tensor of sentences to translate has shape: {to_translate.shape}')\n",
    "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
    "print(f'Tensor of logits has shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3068s\u001b[0m 6s/step - loss: 5.1215 - masked_acc: 0.0994 - masked_loss: 5.1215 - val_loss: 7.7612 - val_masked_acc: 0.0391 - val_masked_loss: 3.8806\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4163s\u001b[0m 8s/step - loss: 0.5553 - masked_acc: 0.9042 - masked_loss: 0.5553 - val_loss: 8.8777 - val_masked_acc: 0.0374 - val_masked_loss: 4.4388\n",
      "Epoch 3/20\n",
      "\u001b[1m413/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m9:00\u001b[0m 6s/step - loss: 0.0136 - masked_acc: 1.0000 - masked_loss: 0.0136"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 13\u001b[0m\n\u001b[0;32m      3\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      4\u001b[0m         train_data\u001b[38;5;241m.\u001b[39mrepeat(),\n\u001b[0;32m      5\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)],\n\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[1;32m---> 13\u001b[0m trained_translator, history \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_and_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslator\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m, in \u001b[0;36mcompile_and_train\u001b[1;34m(model, epochs, steps_per_epoch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_and_train\u001b[39m(model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39mmasked_loss, metrics\u001b[38;5;241m=\u001b[39m[masked_acc, masked_loss])\n\u001b[1;32m----> 3\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Langat Kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def compile_and_train(model, epochs=10, steps_per_epoch=500):\n",
    "    model.compile(optimizer=\"adam\", loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
    "    history = model.fit(\n",
    "        train_data.repeat(),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=50,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "trained_translator, history = compile_and_train(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Next Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
    "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
    "    logits = logits[:, -1, :]\n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "    else:\n",
    "        logits = logits / temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "    logits = tf.squeeze(logits)\n",
    "    next_token = tf.squeeze(next_token)\n",
    "    logit = logits[next_token].numpy()\n",
    "    next_token = tf.reshape(next_token, shape=(1,1))\n",
    "    if next_token == eos_id:\n",
    "        done = True\n",
    "    return next_token, logit, state, done\n",
    "\n",
    "eng_sentence = \"I love languages\"\n",
    "texts = tf.convert_to_tensor(eng_sentence)[tf.newaxis]\n",
    "context = english_vectorizer(texts).to_tensor()\n",
    "context = encoder(context)\n",
    "next_token = tf.fill((1,1), sos_id)\n",
    "state = [tf.random.uniform((1, UNITS)), tf.random.uniform((1, UNITS))]\n",
    "done = False\n",
    "next_token, logit, state, done = generate_next_token(decoder, context, next_token, done, state, temperature=0.5)\n",
    "print(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Translating the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, text, max_length=50, temperature=0.0):\n",
    "    tokens, logits = [], []\n",
    "    text = tf.convert_to_tensor([text])[tf.newaxis]\n",
    "    context = english_vectorizer(text).to_tensor()\n",
    "    context = model.encoder(context)\n",
    "    next_token = tf.fill((1, 1), sos_id)\n",
    "    state = [tf.zeros((1, UNITS)), tf.zeros((1, UNITS))]\n",
    "    done = False\n",
    "    for _ in range(max_length):\n",
    "        try:\n",
    "            next_token, logit, state, done = generate_next_token(\n",
    "                decoder=model.decoder,\n",
    "                context=context,\n",
    "                next_token=next_token,\n",
    "                done=done,\n",
    "                state=state,\n",
    "                temperature=temperature\n",
    "            )\n",
    "        except:\n",
    "            raise Exception(\"Problem generating the next token\")\n",
    "        if done:\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "        logits.append(logit)\n",
    "    tokens = tf.concat(tokens, axis=-1)\n",
    "    translation = tf.squeeze(tokens_to_text(tokens, id_to_word))\n",
    "    translation = translation.numpy().decode()\n",
    "    return translation, logits[-1], tokens\n",
    "\n",
    "temp = 0.0 \n",
    "original_sentence = \"I love languages\"\n",
    "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
    "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")\n",
    "\n",
    "temp = 0.7\n",
    "original_sentence = \"I love languages\"\n",
    "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
    "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Generating Samples and Similarity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, text, n_samples=4, temperature=0.6):\n",
    "    samples, log_probs = [], []\n",
    "    for _ in range(n_samples):\n",
    "        _, logp, sample = translate(model, text, temperature=temperature)\n",
    "        samples.append(np.squeeze(sample.numpy()).tolist())\n",
    "        log_probs.append(logp)\n",
    "    return samples, log_probs\n",
    "\n",
    "samples, log_probs = generate_samples(trained_translator, 'I love languages')\n",
    "for s, l in zip(samples, log_probs):\n",
    "    print(f\"Translated tensor: {s} has logit: {l:.3f}\")\n",
    "\n",
    "def jaccard_similarity(candidate, reference):\n",
    "    candidate_set = set(candidate)\n",
    "    reference_set = set(reference)\n",
    "    common_tokens = candidate_set.intersection(reference_set)\n",
    "    all_tokens = candidate_set.union(reference_set)\n",
    "    overlap = len(common_tokens) / len(all_tokens)\n",
    "    return overlap\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 3, 4]\n",
    "js = jaccard_similarity(l1, l2)\n",
    "print(f\"jaccard similarity between lists: {l1} and {l2} is {js:.3f}\")\n",
    "\n",
    "def rouge1_similarity(candidate, reference):\n",
    "    candidate_word_counts = Counter(candidate)\n",
    "    reference_word_counts = Counter(reference)\n",
    "    overlap = 0\n",
    "    for token in candidate_word_counts.keys():\n",
    "        token_count_candidate = candidate_word_counts[token]\n",
    "        token_count_reference = reference_word_counts[token]\n",
    "        overlap += min(token_count_candidate, token_count_reference)\n",
    "    precision = overlap / len(candidate)\n",
    "    recall = overlap / len(reference)\n",
    "    if precision + recall != 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score\n",
    "    return 0\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 3, 4]\n",
    "r1s = rouge1_similarity(l1, l2)\n",
    "print(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")\n",
    "\n",
    "def average_overlap(samples, similarity_fn):\n",
    "    scores = {}\n",
    "    for index_candidate, candidate in enumerate(samples):\n",
    "        overlap = 0\n",
    "        for index_sample, sample in enumerate(samples):\n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            overlap += sample_overlap\n",
    "        score = overlap / (len(samples) - 1)\n",
    "        score = round(score, 3)\n",
    "        scores[index_candidate] = score\n",
    "    return scores\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "avg_ovlp = average_overlap([l1, l2, l3], jaccard_similarity)\n",
    "print(f\"average overlap between lists: {l1}, {l2} and {l3} using Jaccard similarity is:\\n\\n{avg_ovlp}\")\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "l4 = [5, 6]\n",
    "avg_ovlp = average_overlap([l1, l2, l3, l4], rouge1_similarity)\n",
    "print(f\"average overlap between lists: {l1}, {l2}, {l3} and {l4} using Rouge1 similarity is:\\n\\n{avg_ovlp}\")\n",
    "\n",
    "def weighted_avg_overlap(samples, log_probs, similarity_fn):\n",
    "    scores = {}\n",
    "    for index_candidate, candidate in enumerate(samples):\n",
    "        overlap, weight_sum = 0.0, 0.0\n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "            sample_p = float(np.exp(logp))\n",
    "            weight_sum += sample_p\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            overlap += sample_p * sample_overlap\n",
    "        score = overlap / weight_sum\n",
    "        score = round(score, 3)\n",
    "        scores[index_candidate] = score\n",
    "    return scores\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "log_probs = [0.4, 0.2, 0.5]\n",
    "w_avg_ovlp = weighted_avg_overlap([l1, l2, l3], log_probs, jaccard_similarity)\n",
    "print(f\"weighted average overlap using Jaccard similarity is:\\n\\n{w_avg_ovlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Minimum Bayes Risk Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=jaccard_similarity):\n",
    "    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n",
    "    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n",
    "    decoded_translations = [tokens_to_text(s, id_to_word).numpy().decode('utf-8') for s in samples]\n",
    "    max_score_key = max(scores, key=lambda k: scores[k])\n",
    "    translation = decoded_translations[max_score_key]\n",
    "    return translation, decoded_translations\n",
    "\n",
    "english_sentence = \"I love languages\"\n",
    "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
    "print(\"Translation candidates:\")\n",
    "for c in candidates:\n",
    "    print(c)\n",
    "print(f\"\\nSelected translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
